{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-01`    What is Bayes' theorem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' theorem is a fundamental concept in probability theory, named after the Reverend Thomas Bayes, an 18th-century British mathematician. It provides a way to update our beliefs or knowledge about the probability of an event based on new evidence or information.** \n",
    "\n",
    "`The theorem is expressed mathematically as` :\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $$\n",
    "\n",
    "Where:\n",
    "- $ P(A|B) $ is the probability of event A occurring given that event B has occurred (the posterior probability).\n",
    "- $ P(B|A) $ is the probability of event B occurring given that event A has occurred (the likelihood).\n",
    "- $ P(A) $ and $ P(B) $ are the probabilities of events A and B occurring independently of each other (the prior probabilities).\n",
    "\n",
    "**`In words`, Bayes' theorem tells us how to update our prior belief in the probability of event A happening, given the evidence of event B occurring. It's a way to combine prior knowledge or beliefs with new evidence to obtain a more accurate or informed belief. This theorem is widely used in various fields, including statistics, machine learning, artificial intelligence, and Bayesian inference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-02`    What is the formula for Bayes' theorem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' theorem is a fundamental concept in probability theory and statistics, which describes how to update the probability of a hypothesis given new evidence.**\n",
    "\n",
    "**`The formula for Bayes' theorem` is :**\n",
    "\n",
    "$$ P(A|B) = \\frac{{P(B|A) \\times P(A)}}{{P(B)}} $$\n",
    "\n",
    "**`Where`** :\n",
    "- $ P(A|B) $ is the probability of hypothesis A given the evidence B (posterior probability).\n",
    "- $ P(B|A) $ is the probability of evidence B given that A is true (likelihood).\n",
    "- $ P(A) $ is the prior probability of hypothesis A.\n",
    "- $ P(B) $ is the probability of the evidence B.\n",
    "\n",
    "This formula essentially states that the probability of a hypothesis given some observed evidence is proportional to the likelihood of that evidence given the hypothesis, multiplied by the prior probability of the hypothesis, and divided by the total probability of the evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-03`    How is Bayes' theorem used in practice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' theorem is a fundamental concept in probability theory and statistics, and `it finds applications in various fields including but not limited to` :**\n",
    "\n",
    "1. **Medical Diagnosis -** Bayes' theorem is extensively used in medical diagnosis, particularly in situations where there are multiple diagnostic tests available or when the prior probability of a disease is known. It helps in updating the probability of a disease given the results of a diagnostic test.\n",
    "\n",
    "2. **Spam Filtering -** In email spam filtering, Bayes' theorem is used to classify emails as spam or non-spam based on the occurrence of certain words or phrases. This is commonly known as \"Naive Bayes\" classification.\n",
    "\n",
    "3. **Search Engines -** Bayes' theorem is employed in search engines to rank the relevance of search results to a given query. Algorithms like Bayesian inference are used to improve search accuracy and provide more relevant results to users.\n",
    "\n",
    "4. **Weather Forecasting -** Bayesian methods are utilized in weather forecasting to update the probability of various weather conditions based on new data, such as current atmospheric pressure, temperature, and humidity readings.\n",
    "\n",
    "5. **Machine Learning -** In machine learning, Bayes' theorem serves as the foundation for Bayesian inference methods. It is used in various tasks such as classification, regression, and clustering.\n",
    "\n",
    "6. **Finance -** Bayes' theorem is applied in financial modeling for risk assessment, portfolio management, and predicting stock price movements. It helps in updating beliefs about future events based on new information.\n",
    "\n",
    "7. **A/B Testing -** Bayes' theorem is used in A/B testing to analyze the effectiveness of different versions of a product or service. It helps in determining the probability that one version is better than the other based on observed data.\n",
    "\n",
    "8. **Quality Control -** Bayes' theorem is applied in quality control processes to assess the probability of a product being defective given certain test results or observations.\n",
    "\n",
    "`Overall`, Bayes' theorem provides a powerful framework for reasoning under uncertainty and updating beliefs based on evidence, making it a valuable tool in various practical applications across different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-04`    What is the relationship between Bayes' theorem and conditional probability?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory that describes the probability of an event, based on prior knowledge of conditions that might be related to the event. It is named after the Reverend Thomas Bayes. `Bayes' theorem is based on conditional probability.`\n",
    "\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted by $ P(A|B) $, which reads as \"`the probability of event A given event B.`\" \n",
    "\n",
    "It is calculated as the probability of the intersection of events A and B divided by the probability of event B:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "Bayes' theorem provides a way to revise or update the probability of an event occurring based on new evidence or information. It is stated mathematically as:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "`Where`:\n",
    "- $ P(A|B) $ is the probability of event A occurring given that event B has occurred (the posterior probability).\n",
    "- $ P(B|A) $ is the probability of event B occurring given that event A has occurred (the likelihood).\n",
    "- $ P(A) $ and $ P(B) $ are the probabilities of events A and B occurring independently of each other (the prior probabilities).\n",
    "\n",
    "So, Bayes' theorem essentially shows how to update our belief in the probability of an event (posterior probability) given new evidence (likelihood) and our prior belief (prior probability). It's a powerful tool used in various fields including statistics, machine learning, and artificial intelligence for tasks such as classification, prediction, and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-05`    How do you choose which type of Naive Bayes classifier to use for any given problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing the appropriate type of Naive Bayes classifier depends on several factors such as the nature of the problem, the characteristics of the dataset, and assumptions about the independence of features.** \n",
    "\n",
    "**`Here's a brief overview of the common types of Naive Bayes classifiers and considerations for choosing them` :**\n",
    "\n",
    "1. **Gaussian Naive Bayes -**\n",
    "   - Assumes that continuous features follow a Gaussian (normal) distribution.\n",
    "   - Suitable for classification tasks where features are continuous and can be assumed to be normally distributed.\n",
    "   - Not suitable for features with highly skewed distributions or discrete features.\n",
    "\n",
    "2. **Multinomial Naive Bayes -**\n",
    "   - Appropriate for classification tasks with discrete features (e.g., word counts in text classification).\n",
    "   - Commonly used in text classification, spam filtering, and other document categorization tasks.\n",
    "   - Works well with features that represent counts or frequencies.\n",
    "\n",
    "3. **Bernoulli Naive Bayes -**\n",
    "   - Suitable for binary feature vectors, where features represent presence or absence (e.g., word presence/absence in document classification).\n",
    "   - Often used in text classification tasks with binary feature representations.\n",
    "   - Each feature is assumed to be generated from a Bernoulli distribution.\n",
    "\n",
    "4. **Complement Naive Bayes -**\n",
    "   - Particularly useful for imbalanced datasets, where classes are disproportionately represented.\n",
    "   - It's designed to correct the imbalances in class distributions.\n",
    "   - Especially effective in text classification tasks where some classes may dominate the training data.\n",
    "\n",
    "5. **Categorical Naive Bayes -**\n",
    "   - Similar to Multinomial Naive Bayes, but suitable for categorical features with more than two levels.\n",
    "   - Appropriate when dealing with features that are categorical rather than strictly counts.\n",
    "   - Often used in recommendation systems or other tasks involving categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`When selecting a Naive Bayes classifier for a given problem, consider the following` :**\n",
    "\n",
    "- **Data Distribution -** Understand the distribution of your features. If your features are continuous and normally distributed, Gaussian Naive Bayes might be appropriate. For discrete features, consider Multinomial or Bernoulli Naive Bayes.\n",
    "  \n",
    "- **Feature Independence -** Assess the independence assumption of features. Naive Bayes classifiers assume that features are conditionally independent given the class label. While this assumption is rarely met in real-world data, Naive Bayes can still perform well if the violation of independence is not severe.\n",
    "\n",
    "- **Nature of the Problem -** Consider the nature of your classification problem. For example, if you're working with text data, Multinomial or Bernoulli Naive Bayes might be more suitable. If your dataset has imbalanced class distributions, consider using Complement Naive Bayes.\n",
    "\n",
    "- **Experimentation and Validation -** Experiment with different Naive Bayes variants and evaluate their performance using cross-validation or hold-out validation on your dataset. Choose the variant that yields the best performance metrics for your specific problem.\n",
    "\n",
    "`Overall`, the choice of Naive Bayes classifier should be guided by the characteristics of your data and the assumptions that best match your problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Q.No-06`    Assignment :-**\n",
    "\n",
    "**You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4.**\n",
    "\n",
    "**The following table shows the frequency of each feature value for each class :**\n",
    "\n",
    "| Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
    "|-------|------|------|------|------|------|------|------|\n",
    "| A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
    "| B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n",
    "\n",
    "**Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Given` :** **The prior probabilities are equal**, $P(A) = P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`According to Question` -**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To classify the new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we will compute the posterior probability for each class given the features and then choose the class with the highest posterior probability.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The posterior probability for a class given the features can be calculated using `Bayes' theorem` -**\n",
    "\n",
    "$$ P(A|X_1, X_2) = \\frac{P(X_1, X_2|A) \\times P(A)}{P(X_1, X_2)} $$\n",
    "\n",
    "$$ P(B|X_1, X_2) = \\frac{P(X_1, X_2|B) \\times P(B)}{P(X_1, X_2)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Therefor`, $ P(A) = P(B)$ **and** $P(X_1,X_2)$ **are same in both the equation. So, we can ignore them in the comparison.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The posterior probability for a class given the features can be calculated using `Bayes' theorem` -**\n",
    "\n",
    "$$ P(Class=A|X_1, X_2) = P(X_1, X_2|A) = P(X_1|A) \\times P(X_2|A) $$\n",
    "\n",
    "$$ P(Class=B|X_1, X_2) = P(X_1, X_2|B) = P(X_1|B) \\times P(X_2|B) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The frequency of each feature value for each class -**\n",
    "\n",
    "| Class | X1=1 | X1=2 | X1=3 | X2=1 | X2=2 | X2=3 | X2=4 |\n",
    "|-------|------|------|------|------|------|------|------|\n",
    "| A     | 3    | 3    | 4    | 4    | 3    | 3    | 3    |\n",
    "| B     | 2    | 2    | 1    | 2    | 2    | 2    | 3    |\n",
    "\n",
    "$$Total ~frequency ~of ~class ~A = 23 ~~and ~~Total ~frequency ~of ~class ~B = 14$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To calculate** $P(Class=A|X_1=3, X_2=4)$ **and** $P(Class=B|X_1=3, X_2=4)$, **we will use the frequency counts provided in the table.**\n",
    "\n",
    "$$P(X_1=3|Class=A) =  \\frac{4}{23}$$\n",
    "\n",
    "$$P(X_2=4|Class=A) =  \\frac{3}{23}$$\n",
    "\n",
    "$$P(X_1=3|Class=B) =  \\frac{1}{14}$$\n",
    "\n",
    "$$P(X_2=4|Class=B) =  \\frac{3}{14}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Now`, let's compute the posterior probabilities -**\n",
    "\n",
    "$$P(Class=A|X_1, X_2) = P(X_1=3|Class=A) \\times P(X_2=4|Class=A) = \\frac{4}{23}  \\times \\frac{3}{23} = 0.02268431001890359 \\approx 0..023$$\n",
    "\n",
    "$$P(Class=B|X_1, X_2) = P(X_1=3|Class=B) \\times P(X_2=4|Class=B) = \\frac{1}{14}  \\times \\frac{3}{14} = 0.01530612244897959 \\approx 0..015$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Since`** $ P(Class=A|X_1, X_2) > P(Class=B|X_1, X_2) $, **Naive Bayes would predict that the new instance belongs to class A.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
